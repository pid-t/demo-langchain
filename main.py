import math
import os
import time

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.runnables import RunnableConfig
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_tavily import TavilySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

load_dotenv()


def main():
    memory = MemorySaver()
    model = init_chat_model(
        model=os.getenv("MODEL"),
        model_provider="openai",
        base_url=os.getenv("BASE_URL"),
        api_key=os.getenv("API_KEY"),
    )

    search = TavilySearch(max_results=2, tavily_api_key=os.getenv("TAVILY_API_KEY"))
    tools = [search]

    agent_executor = create_react_agent(model, tools, checkpointer=memory)
    config: RunnableConfig = {"configurable": {"thread_id": "abc123"}}

    print("ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„å¤©æ°”åŠ©æ‰‹(AI ç‰ˆ).")
    while True:
        user_input = input("ğŸ‘©â€ğŸ¤ ï¼š")
        user_input = user_input.strip()
        if not user_input:
            continue

        if user_input.lower() in ["é€€å‡º", "exit"]:
            print("å†è§ï¼")
            break

        print("ğŸ¤– :", end="")
        input_message = {"role": "user", "content": user_input}
        # streaming output
        for step, metadata in agent_executor.stream({"messages": [input_message]}, config, stream_mode="messages"):
            if metadata["langgraph_node"] == "agent" and (text := step.text()):
                print(text, end="")
        print()


def create_vector_store_in_batches(documents, embedding_model, batch_size=100, delay_seconds=1):
    # --- 2. å®šä¹‰å…è´¹å¥—é¤çš„å¸¸é‡ ---
    # Google AI Studio å…è´¹å¥—é¤çš„é€Ÿç‡é™åˆ¶
    FREE_TIER_RPM = 60
    # embedding-001 æ¨¡å‹çš„æœ€å¤§æ‰¹å¤„ç†å¤§å°
    MAX_BATCH_SIZE = 10
    # è®¡ç®—ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´çš„å®‰å…¨ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    # (60ç§’ / 60æ¬¡è¯·æ±‚) = 1ç§’/æ¬¡ã€‚å¢åŠ 0.1ç§’ä½œä¸ºç½‘ç»œå»¶è¿Ÿç­‰çš„ç¼“å†²ã€‚
    SECONDS_PER_REQUEST = 5

    print(
        f"é€Ÿç‡é™åˆ¶ç­–ç•¥ï¼šæ¯åˆ†é’Ÿ {FREE_TIER_RPM} æ¬¡è¯·æ±‚ï¼Œæ‰¹å¤„ç†å¤§å° {MAX_BATCH_SIZE}ï¼Œæ¯æ¬¡è¯·æ±‚åç­‰å¾… {SECONDS_PER_REQUEST:.2f} ç§’ã€‚"
    )

    vector_store = None
    total_docs = len(documents)
    total_batches = math.ceil(total_docs / MAX_BATCH_SIZE)
    print(f"\næ–‡æ¡£æ€»æ•°: {total_docs}ï¼Œå°†åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†ã€‚")
    for i in range(0, total_docs, MAX_BATCH_SIZE):
        batch = documents[i : i + MAX_BATCH_SIZE]
        current_batch_num = (i // MAX_BATCH_SIZE) + 1

        print(f"--> æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {current_batch_num}/{total_batches} (æ–‡æ¡£ {i + 1} åˆ° {i + len(batch)})...")

        if vector_store is None:
            # å¯¹äºç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼Œä½¿ç”¨ from_documents åˆ›å»ºå‘é‡å­˜å‚¨
            # è¿™æ˜¯é¦–æ¬¡ API è°ƒç”¨
            vector_store = InMemoryVectorStore.from_documents(documents=batch, embedding=embedding_model)
        else:
            # å¯¹äºåç»­æ‰¹æ¬¡ï¼Œä½¿ç”¨ add_documents æ·»åŠ åˆ°ç°æœ‰å­˜å‚¨
            # è¿™æ˜¯åç»­çš„ API è°ƒç”¨
            vector_store.add_documents(documents=batch)

        print(f"    æ‰¹æ¬¡ {current_batch_num} å¤„ç†å®Œæˆã€‚")

        if current_batch_num < total_batches:
            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼Œåˆ™ç­‰å¾…ä»¥éµå®ˆé€Ÿç‡é™åˆ¶
            print(f"    ç­‰å¾… {SECONDS_PER_REQUEST:.2f} ç§’...")
            time.sleep(SECONDS_PER_REQUEST)

    print("\næ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæ¯•ï¼Œå‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
    return vector_store


def main2():
    print("start to test PyPDFLoader.")
    file_path = "./nke-10k-2023.pdf"
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    print(len(docs))
    print(f"{docs[0].page_content[:200]}\n")
    print(docs[0].metadata)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, add_start_index=True)
    all_splits = text_splitter.split_documents(docs)
    print(len(all_splits))
    os.environ["GOOGLE_API_KEY"] = "AIzaSyCpI7qAx60fMpZolNMWiLrKwQyFLKSa6KA"
    embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
    vector_store = create_vector_store_in_batches(all_splits, embeddings, batch_size=5, delay_seconds=10)
    results = vector_store.similarity_search("How many distribution centers does Nike have in the US?")
    print(results[0])


if __name__ == "__main__":
    # main()
    main2()
